# Map-Reduce RAG Implementation Summary

## ‚úÖ Backend Implementation Complete

### Files Created

1. **`backend/app/models/database.py`** - Updated
   - Added `ProcessingJob` model for tracking async jobs
   - Updated `Message` model with `job_id` field

2. **`backend/app/models/schemas.py`** - Updated
   - Added `ProcessingJobStatus` schema
   - Updated `ChatResponse` schema with job fields

3. **`backend/app/services/intent_service.py`** - NEW
   - Intent classification using LLM
   - Estimates processing time and determines async vs quick query routing
   - Supports aggregation, full_folder_summary, and filtered_aggregation intents

4. **`backend/app/services/mapreduce_service.py`** - NEW
   - Map-reduce processing for large-scale queries
   - Parallel batch processing with configurable concurrency
   - Programmatic aggregation with detailed breakdowns
   - Progress tracking and error handling

5. **`backend/app/services/chat_service.py`** - Updated
   - Added async query handling with `_handle_async_query()`
   - Added quick query handling with `_handle_quick_query()`
   - Background job processing with `_process_job_in_background()`
   - Intent classification integration

6. **`backend/app/api/v1/endpoints/chat.py`** - Updated
   - Updated chat endpoint to support BackgroundTasks
   - Added `/jobs/{job_id}` endpoint to get job status
   - Added `/jobs` endpoint to list jobs
   - Added `/jobs/{job_id}` DELETE endpoint to cancel jobs

7. **`backend/MIGRATION_INSTRUCTIONS.md`** - NEW
   - Database migration instructions
   - SQL scripts for creating tables and indexes

### Architecture Overview

```
User Query ‚Üí Intent Classification
    ‚îú‚îÄ Quick Query (<5s)
    ‚îÇ   ‚îî‚îÄ Standard RAG (top-k retrieval)
    ‚îÇ       ‚îî‚îÄ Return immediately
    ‚îÇ
    ‚îî‚îÄ Long Query (>5s)
        ‚îî‚îÄ Create ProcessingJob
            ‚îú‚îÄ Return job_id immediately
            ‚îú‚îÄ Process in background
            ‚îÇ   ‚îú‚îÄ Map phase (parallel batches)
            ‚îÇ   ‚îú‚îÄ Reduce phase (aggregation)
            ‚îÇ   ‚îî‚îÄ Synthesis (LLM summary)
            ‚îî‚îÄ Store results in DB
```

### Key Features

‚úÖ **Intent-Driven Routing**
- LLM classifies query intent automatically
- Estimates processing time based on folder size
- Routes to async or quick processing

‚úÖ **Asynchronous Processing**
- Long queries don't block the user
- Background task processing with FastAPI
- Progress tracking at 5-batch intervals

‚úÖ **Map-Reduce Architecture**
- Scales to thousands of items
- Parallel batch processing (10 concurrent by default)
- Smart batching (targets ~10 chunks per batch)

‚úÖ **Detailed Aggregation**
- Programmatic aggregation of numeric values
- Breakdown by category and month
- Confidence scoring

‚úÖ **Error Resilience**
- Retry logic for failed batches
- Graceful handling of partial failures
- Detailed error reporting

‚úÖ **User Experience**
- Can navigate away and return
- Real-time progress updates
- Transparent result breakdown

## üîÑ Frontend Implementation Needed

The following frontend components need to be created:

### 1. Update API Client

**File: `frontend/src/api/chat.ts`**

```typescript
export interface ChatResponse {
  response: string;
  conversation_id: string;
  job_id?: string;
  job_status?: 'queued' | 'processing' | 'completed' | 'failed';
  estimated_completion_seconds?: number;
  sources: Array<{...}>;
  context_count: number;
  hashtag_info?: any;
}

export interface ProcessingJobStatus {
  job_id: string;
  status: string;
  progress: number;
  current_phase: string;
  processed_items: number;
  total_items: number;
  estimated_completion_seconds?: number;
  result?: any;
  aggregation_details?: any;
  error_message?: string;
  started_at: string;
  completed_at?: string;
}

export async function getJobStatus(jobId: string): Promise<ProcessingJobStatus> {
  const response = await fetch(`/api/v1/chat/jobs/${jobId}`, {
    headers: { 'Authorization': `Bearer ${getToken()}` }
  });
  return response.json();
}

export async function cancelJob(jobId: string): Promise<void> {
  await fetch(`/api/v1/chat/jobs/${jobId}`, {
    method: 'DELETE',
    headers: { 'Authorization': `Bearer ${getToken()}` }
  });
}
```

### 2. Job Progress Indicator Component

**File: `frontend/src/components/JobProgressIndicator.tsx`**

Features:
- Polls job status every 2 seconds
- Displays progress bar (0-100%)
- Shows current phase (queued, map, reduce, synthesis, complete)
- Shows items processed count
- Calls `onComplete` when job finishes
- Handles errors gracefully

### 3. Aggregation Details Component

**File: `frontend/src/components/AggregationDetails.tsx`**

Features:
- Expandable/collapsible section
- Summary (total, count, average)
- Processing info (items processed, batches, confidence)
- Breakdown by category table
- Breakdown by month table
- Top items table

### 4. Chat Interface Integration

**File: `frontend/src/components/Chat.tsx`**

Updates needed:
- Handle `job_id` in ChatResponse
- Display JobProgressIndicator when job is created
- Poll for job completion
- Replace progress indicator with final result
- Display aggregation details when available

## üìã Testing Checklist

### Backend Tests

- [ ] Test intent classification with various query types
- [ ] Test map-reduce processing with small dataset (< 50 items)
- [ ] Test map-reduce processing with large dataset (> 100 items)
- [ ] Test async job creation and background processing
- [ ] Test job status polling
- [ ] Test job cancellation
- [ ] Test error handling (empty folder, all batches fail)
- [ ] Test partial failures (some batches fail)

### Frontend Tests

- [ ] Test job progress indicator display
- [ ] Test progress polling and updates
- [ ] Test job completion and result display
- [ ] Test aggregation details expansion
- [ ] Test navigation away and return
- [ ] Test multiple concurrent jobs
- [ ] Test error display
- [ ] Test network error handling during polling

## üöÄ Deployment Steps

### 1. Database Migration

```bash
cd backend

# Option A: Using Alembic (if set up)
alembic revision --autogenerate -m "Add processing_jobs table"
alembic upgrade head

# Option B: Direct SQL (see MIGRATION_INSTRUCTIONS.md)
```

### 2. Backend Deployment

```bash
# Test the new services
python -m pytest app/tests/

# Deploy backend with updated code
# Make sure environment has all dependencies
pip install -r requirements.txt

# Restart backend server
```

### 3. Frontend Deployment

```bash
cd frontend

# Install dependencies (if any new ones)
npm install

# Build frontend
npm run build

# Deploy
```

### 4. Verification

```bash
# Test quick query
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message": "What is X?", "user_id": "..."}'

# Test aggregation query (should return job_id)
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message": "What is the total of all items in #folder?", "user_id": "..."}'

# Check job status
curl -X GET http://localhost:8000/api/v1/chat/jobs/{job_id} \
  -H "Authorization: Bearer $TOKEN"
```

## ‚öôÔ∏è Configuration

### Environment Variables

No new environment variables required. Uses existing:
- Database connection (from existing config)
- OpenAI API key (from existing config)

### Tuning Parameters

In `backend/app/services/mapreduce_service.py`:

```python
TARGET_CHUNKS_PER_BATCH = 10  # Chunks per batch
MAX_CONCURRENT_MAP_CALLS = 10  # Parallel processing limit
MAP_RETRY_ATTEMPTS = 2  # Retry failed batches
MAX_JOB_DURATION_SECONDS = 600  # 10 minute timeout
```

In `backend/app/services/intent_service.py`:

```python
QUICK_QUERY_THRESHOLD_SECONDS = 5  # Switch to async above this
ITEMS_PER_SECOND_ESTIMATE = 10  # Processing speed estimate
```

## üéØ Performance Optimizations

For folders with 1000+ items:

1. **Increase parallelism:**
   ```python
   MAX_CONCURRENT_MAP_CALLS = 20
   ```

2. **Larger batch size:**
   ```python
   TARGET_CHUNKS_PER_BATCH = 15
   ```

3. **Use cheaper model for map phase:**
   ```python
   MAP_MODEL = "gpt-4o-mini"
   REDUCE_MODEL = "gpt-4o"
   ```

## üìä Monitoring

Key metrics to monitor:

- Job processing times (`processing_jobs.actual_duration_seconds`)
- Job success/failure rates (`processing_jobs.status`)
- Batch failure rates (`processing_jobs.failed_batches`)
- Intent classification accuracy (manual review)
- User satisfaction with aggregation results

## üêõ Troubleshooting

### Jobs stuck in "queued" status
- Check backend server logs
- Verify BackgroundTasks is working
- Check database connection

### All batches failing
- Check OpenAI API key and rate limits
- Review map prompt format
- Check chunk content quality

### Incorrect aggregations
- Review map phase extraction
- Check LLM response parsing
- Verify programmatic aggregation logic

### Frontend not updating
- Check polling interval (should be 2 seconds)
- Verify job_id is being stored correctly
- Check for console errors

## üìö Next Steps

1. **Implement Frontend Components**
   - Create JobProgressIndicator
   - Create AggregationDetails
   - Update Chat interface

2. **Run Database Migration**
   - Follow MIGRATION_INSTRUCTIONS.md

3. **Test End-to-End**
   - Create test folder with sample data
   - Test various query types
   - Verify results accuracy

4. **Deploy to Production**
   - Run backend tests
   - Deploy backend
   - Deploy frontend
   - Monitor initial usage

## üìù Documentation

- See `IMPLEMENTATION_PLAN_MAP_REDUCE_RAG.md` for detailed architecture
- See `MIGRATION_INSTRUCTIONS.md` for database migration
- API endpoints documented in code with docstrings
- Frontend components will have inline documentation

---

**Implementation Status: Backend 100% Complete, Frontend 0% Complete**

The backend is fully functional and ready for testing. Once the database migration is run, you can test the async job processing with API calls. The frontend components need to be implemented to provide the user interface.
